# Financial Data Scrapper

Este projeto automatiza a extraÃ§Ã£o, transformaÃ§Ã£o e carga (ETL) de dados financeiros de fontes web utilizando **Selenium remoto** (Cloud Run), **Apache Airflow** e APIs Python modernas. O objetivo Ã© coletar, processar e armazenar dados financeiros de forma escalÃ¡vel e segura, integrando com Google Cloud Platform.
---
# Arquitetura
![alt text](scripts/image.png)

---

## ğŸš€ Principais Funcionalidades

- **ExtraÃ§Ã£o de dados financeiros** de sites como Bloomberg usando Selenium remoto.
- **OrquestraÃ§Ã£o de ETL** com Apache Airflow (DAGs customizadas).
- **Deploy automatizado** no Google Cloud Run via Cloud Build.
- **Provisionamento de infraestrutura com Terraform** (Cloud Build, Cloud Run, APIs, IAM, etc).
- **AutenticaÃ§Ã£o segura** entre serviÃ§os usando Identity Token do Google.
- **ConfiguraÃ§Ã£o de conexÃµes Airflow** via YAML para fÃ¡cil integraÃ§Ã£o.

---

## ğŸ§© Como funciona

1. **Airflow DAG** dispara uma chamada HTTP para a API no Cloud Run.
2. **API FastAPI** executa o pipeline ETL, que:
   - Usa Selenium remoto (Cloud Run) para raspar dados.
   - Transforma os dados em DataFrames (Polars).
   - Carrega os dados em BigQuery (ou outro destino).
3. **Selenium remoto** Ã© autenticado via Identity Token (dinÃ¢mico no Cloud Run, variÃ¡vel local para testes).
4. **Deploy automÃ¡tico**: Push no GitHub aciona o Cloud Build, que faz build, push e deploy no Cloud Run.
5. **Infraestrutura gerenciada via Terraform**: Cloud Build triggers, Cloud Run, APIs, IAM, BigQuery, etc.

---

## âš™ï¸ Como rodar localmente

1. **Clone o repositÃ³rio**
2. **Configure o `.env`** com:
   ```
   SELENIUM_URL=https://<seu-endpoint-selenium>/wd/hub
   IDENTITY_TOKEN=<token_gerado_pelo_gcloud>
   GOOGLE_APPLICATION_CREDENTIALS=/caminho/para/sua/key.json
   ```
3. **Instale as dependÃªncias**
   ```bash
   pip install -r requirements.txt
   ```
4. **Rode a API**
   ```bash
   uvicorn api.main:app --host 0.0.0.0 --port 8080
   ```
5. **(Opcional) Rode o Airflow localmente**
   ```bash
   astro dev start
   ```

---

## ğŸ§± Provisionamento de Infraestrutura com Terraform

> â„¹ï¸ **InformaÃ§Ã£o:**  
> A infraestrutura provisionada via Terraform **ainda estÃ¡ em processo de estruturaÃ§Ã£o**. Algumas configuraÃ§Ãµes, integraÃ§Ãµes ou recursos podem sofrer ajustes e melhorias nas prÃ³ximas versÃµes do projeto.

1. **Configure as variÃ¡veis em `terraform/variables.tf`** (exemplo):
   ```
   project_id         = "seu-projeto-gcp"
   region             = "us-central1"
   github_owner       = "seu-usuario-ou-org"
   github_repo        = "financial_data_scrapper"
   github_full_repo   = "https://github.com/seu-usuario-ou-org/financial_data_scrapper"
   github_app_installation_id = "SEU_ID"
   secret             = "projects/SEU_PROJECT/secrets/SEU_SECRET/versions/latest"
   trigger_name       = "test-trigger"
   selenium           = "https://<seu-endpoint-selenium>/wd/hub"
   service_account    = "projects/-/serviceAccounts/NUMERO_DO_PROJETO@cloudbuild.gserviceaccount.com"
   ```

2. **Inicialize o Terraform**
   ```bash
   cd terraform
   terraform init
   ```

3. **Aplique a infraestrutura**
   ```bash
   terraform apply
   ```
   Isso irÃ¡:
   - Ativar as APIs necessÃ¡rias no GCP.
   - Criar conexÃµes e repositÃ³rios do Cloud Build.
   - Criar triggers automatizados para build/deploy.
   - Configurar permissÃµes e service accounts.

4. **Dispare builds manualmente (opcional)**
   ```bash
   gcloud builds triggers run test-trigger --branch=main
   ```

---

## â˜ï¸ Deploy automÃ¡tico no Cloud Run

- O arquivo [`selenium-trigger.cloudbuild.yaml`](selenium-trigger.cloudbuild.yaml) define o pipeline de build e deploy.
- O deploy Ã© feito automaticamente a cada push via Cloud Build Trigger.
- O serviÃ§o usa uma Service Account com permissÃµes para gerar Identity Tokens.

---

## ğŸ“ Principais arquivos

- `api/main.py` â€” Entrypoint da API FastAPI.
- `api/utils/selenium_helper.py` â€” Helper para scraping com Selenium remoto.
- `api/utils/selenium_remote_connection_v2.py` â€” ConexÃ£o autenticada com Selenium remoto.
- `dags/financial_data_scraper/dag_financial_scraper.py` â€” DAG do Airflow para orquestraÃ§Ã£o.
- `dockerfile.api` â€” Dockerfile para build da API.
- `selenium-trigger.cloudbuild.yaml` â€” Pipeline de CI/CD para Cloud Run.
- `include/airflow_connections.yaml` â€” ConexÃµes automÃ¡ticas do Airflow.

---

## ğŸ›¡ï¸ SeguranÃ§a

- Tokens de identidade sÃ£o usados para autenticaÃ§Ã£o segura entre serviÃ§os.
- VariÃ¡veis sensÃ­veis sÃ£o passadas via variÃ¡veis de ambiente e nÃ£o hardcoded.

## ğŸ“š ReferÃªncias

- [DocumentaÃ§Ã£o Cloud Run](https://cloud.google.com/run/docs)
- [DocumentaÃ§Ã£o Selenium Grid](https://www.selenium.dev/documentation/grid/)
- [DocumentaÃ§Ã£o Airflow](https://airflow.apache.org/docs/)
- [DocumentaÃ§Ã£o Cloud Build](https://cloud.google.com/build/docs)
- [DocumentaÃ§Ã£o Terraform Google Modules](https://github.com/terraform-google-modules/terraform-google-project-factory)
- [Artigo sobre scrapper com cloud_run](https://www.roelpeters.be/how-to-deploy-a-scraping-script-and-selenium-in-google-cloud-run/)

---

## ğŸ“¦ Estrutura do Projeto

```
financial_data_scrapper/
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ selenium_helper.py
â”‚   â”‚   â””â”€â”€ selenium_remote_connection_v2.py
â”‚   â””â”€â”€ pipeline/
â”‚       â””â”€â”€ extractors/
â”‚           â””â”€â”€ bloomberg_commodity.py
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ financial_data_scraper/
â”‚       â””â”€â”€ dag_financial_scraper.py
â”œâ”€â”€ include/
â”‚   â””â”€â”€ airflow_connections.yaml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ dockerfile.api
â”œâ”€â”€ selenium-trigger.cloudbuild.yaml
â”œâ”€â”€ terraform/
â”‚   â”œâ”€â”€ main.tf
â”‚   â”œâ”€â”€ cloud_build/
â”‚   â”‚   â””â”€â”€ main.tf
â”‚   â””â”€â”€ ...
â””â”€â”€ .env
```

**DÃºvidas?**  
Abra uma issue ou entre em contato!
